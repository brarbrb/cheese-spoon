import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import re
from typing import List, Dict, Optional
import warnings

MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
# Alternative lightweight models:
# "HuggingFaceTB/SmolLM2-1.7B-Instruct" - good multilingual support
# "Qwen/Qwen2.5-1.5B-Instruct" - excellent for Hebrew

USE_QUANTIZATION = False  # Set to False for 1B models (they're small enough)

# ============================================================================
# PROMPT TEMPLATES
# ============================================================================

REVIEW_SUMMARY_PROMPT = """◊ê◊™◊î ◊¢◊ï◊ñ◊® ◊ê◊ß◊ì◊û◊ô ◊©◊û◊†◊™◊ó ◊ë◊ô◊ß◊ï◊®◊ï◊™ ◊©◊ú ◊°◊ò◊ï◊ì◊†◊ò◊ô◊ù ◊¢◊ú ◊ß◊ï◊®◊°◊ô◊ù.



◊û◊©◊ô◊û◊™◊ö:

1. ◊ë◊ó◊® ◊¢◊ì 4 ◊¶◊ô◊ò◊ï◊ò◊ô◊ù ◊ß◊¶◊®◊ô◊ù ◊ï◊û◊ô◊ô◊¶◊í◊ô◊ù ◊û◊î◊ë◊ô◊ß◊ï◊®◊ï◊™ ◊©◊û◊™◊ê◊®◊ô◊ù ◊ê◊™ ◊î◊ß◊ï◊®◊° ◊ë◊ê◊ï◊§◊ü ◊õ◊ú◊ú◊ô (◊ú◊ê ◊°◊§◊¶◊ô◊§◊ô◊™ ◊¢◊ú ◊û◊®◊¶◊î ◊ê◊ï ◊û◊™◊®◊í◊ú ◊°◊§◊¶◊ô◊§◊ô).

2. ◊™◊ü ◊¢◊ì◊ô◊§◊ï◊™ ◊ú◊ë◊ô◊ß◊ï◊®◊ï◊™ ◊©◊†◊®◊ê◊ï◊™ ◊¢◊ì◊õ◊†◊ô◊ï◊™ ◊ô◊ï◊™◊® (◊û◊ï◊§◊ô◊¢◊ï◊™ ◊ë◊™◊ó◊ô◊ú◊™ ◊î◊®◊©◊ô◊û◊î).

3. ◊õ◊™◊ï◊ë ◊°◊ô◊õ◊ï◊ù ◊©◊ú 2-3 ◊©◊ï◊®◊ï◊™ ◊¢◊ú ◊î◊ó◊ï◊ï◊ô◊î ◊î◊õ◊ú◊ú◊ô◊™ ◊©◊ú ◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ù - ◊¢◊ú ◊û◊î ◊ú◊î◊ô◊ñ◊î◊®, ◊û◊î ◊ó◊©◊ï◊ë ◊ú◊ì◊¢◊™.


◊ì◊ï◊í◊û◊ê:
◊î◊ë◊ô◊ß◊ï◊®◊ï◊™:
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊õ◊ú ◊©◊ë◊ï◊¢ ◊î◊®◊¶◊ê◊î ◊¢◊ù ◊†◊ï◊©◊ê ◊ó◊ì◊©, ◊û◊ì◊ë◊®◊ô◊ù ◊¢◊ú ◊†◊ï◊©◊ê◊ô◊ù ◊ï◊ì◊†◊ô◊ù ◊ë◊î◊ù, ◊û◊®◊¢◊†◊ü ◊ê◊™ ◊î◊©◊ë◊ï◊¢ ◊û◊ê◊ú◊í◊ë◊®◊î ◊ï◊ì◊ô◊°◊ß◊®◊ò◊ô◊™. ◊î◊®◊¶◊ê◊ï◊™ ◊©◊û◊ì◊ë◊®◊ï◊™ ◊™◊õ◊ú◊° ◊¢◊ú ◊î◊™◊§◊ß◊ô◊ì ◊ë◊™◊¢◊©◊ô◊ô◊î, ◊ê◊ó◊ú◊î ◊ú◊°◊û◊°◊ò◊® ◊®◊ê◊©◊ï◊ü.

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊ê◊ô◊ü ◊¢◊ú ◊†◊ó◊û◊ô◊î, ◊ó◊ë◊ú ◊©◊ú◊ê ◊ô◊¶◊ê ◊ú◊†◊ï ◊ú◊ú◊û◊ï◊ì ◊ê◊ô◊™◊ï ◊ë◊ú◊ô◊ô◊ë.

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊õ◊ú ◊©◊ë◊ï◊¢ ◊¢◊ë◊ï◊ì◊î ◊ß◊ú◊ô◊ú◊î ◊ë◊ñ◊ï◊í◊ï◊™, ◊ë◊°◊ï◊£ ◊î◊°◊û◊°◊ò◊® ◊û◊¢◊ë◊ì◊™ ◊®◊ï◊ë◊ï◊ò◊ô◊ß◊î (◊í◊ù ◊ë◊ñ◊ï◊ù).
* ◊ê◊™ ◊î◊¶◊ô◊ï◊ü ◊©◊ú ◊î◊¢◊ë◊ï◊ì◊ï◊™ ◊™◊î◊ô◊ï ◊û◊ï◊õ◊†◊ô◊ù ◊ú◊ß◊ë◊ú ◊ê◊ó◊®◊ô ◊î◊¶◊ô◊ï◊ü ◊©◊ú ◊î◊û◊ë◊ó◊ü

◊î◊û◊ë◊ó◊ü: ◊î◊ñ◊õ◊ô◊® ◊ú◊ô ◊†◊©◊õ◊ó◊ï◊™ ◊©◊ú ◊û◊ë◊ó◊†◊ô◊ù ◊û◊ô◊û◊ô ◊î◊™◊ô◊õ◊ï◊ü

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ê◊ó◊ú◊î ◊ß◊ï◊®◊°, ◊û◊¢◊ú◊î ◊û◊û◊ï◊¶◊¢ ◊ú◊°◊û◊°◊ò◊® ◊®◊ê◊©◊ï◊ü, ◊ï◊û◊¢◊©◊ô◊® ◊ê◊™ ◊î◊ô◊ì◊¢ ◊ú◊í◊ë◊ô ◊î◊™◊ó◊ï◊û◊ô◊ù ◊©◊ú ◊û◊î◊†◊ì◊° ◊™◊¢◊©◊ô◊ô◊î ◊ë◊¢◊™◊ô◊ì
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊õ◊ú ◊©◊ë◊ï◊¢ ◊û◊ì◊ë◊®◊ô◊ù ◊¢◊ú ◊†◊ï◊©◊ê ◊ê◊ó◊®. ◊û◊û◊© ◊†◊î◊†◊ô◊™◊ô ◊ë◊î◊®◊¶◊ê◊ï◊™! ◊ê◊ó◊ú◊î ◊î◊§◊ï◊í◊î ◊û◊©◊ê◊® ◊î◊ß◊ï◊®◊°◊ô◊ù ◊î◊ô◊ï◊™◊® ◊û◊™◊û◊ò◊ô◊ô◊ù ◊ë◊û◊î◊ú◊ö ◊î◊©◊ë◊ï◊¢. ◊ê◊°◊£ ◊û◊®◊¶◊î ◊û◊¢◊ï◊ú◊î ◊ï◊®◊ï◊ê◊ô◊ù ◊©◊ê◊õ◊§◊™ ◊ú◊ï ◊û◊î◊ß◊ï◊®◊° ◊ï◊©◊î◊ï◊ê ◊ê◊ï◊î◊ë ◊ê◊™ ◊î◊™◊ó◊ï◊ù.

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊ú◊ê ◊î◊ú◊õ◊™◊ô ◊ú◊™◊®◊í◊ï◊ú◊ô◊ù ◊î◊ê◊û◊™, ◊î◊°◊§◊ô◊ß ◊ú◊ô ◊ú◊¢◊ë◊ï◊® ◊¢◊ú ◊î◊û◊¶◊í◊™ ◊ï◊î◊§◊™◊®◊ï◊†◊ï◊™ ◊ë◊ë◊ô◊™.

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊í◊ô◊ú◊ô◊ï◊ü ◊ú◊î◊í◊©◊î ◊õ◊ú ◊©◊ë◊ï◊¢ ◊ë◊ñ◊ï◊í◊ï◊™/◊ô◊ó◊ô◊ì◊ô◊ù. ◊ú◊ê ◊ß◊©◊î ◊ë◊û◊ô◊ï◊ó◊ì :)

◊î◊û◊ë◊ó◊ü: ◊ê◊†◊ô ◊õ◊ü ◊ó◊ï◊©◊ë◊™ ◊©◊î◊ô◊î ◊û◊ë◊ó◊ü ◊î◊ï◊í◊ü ◊û◊ë◊ó◊ô◊†◊™ ◊®◊ï◊ë ◊î◊©◊ê◊ú◊ï◊™ ◊©◊†◊©◊ê◊ú◊ï ◊ë◊ï ◊ï◊î◊ù ◊î◊¶◊ú◊ô◊ó◊ï ◊ú◊õ◊°◊ï◊™ ◊õ◊û◊¢◊ò ◊ê◊™ ◊õ◊ú ◊î◊ó◊ï◊û◊® ◊î◊†◊ú◊û◊ì. ◊¢◊ù ◊ñ◊ê◊™, ◊î◊ô◊ï ◊ú◊ê ◊û◊¢◊ò ◊°◊¢◊ô◊§◊ô◊ù ◊ò◊®◊ô◊ß◊ô◊ô◊ù ◊ï◊û◊ë◊ú◊ë◊ú◊ô◊ù ◊ï◊¢◊ô◊ß◊® ◊î◊ë◊¢◊ô◊î ◊î◊ô◊î ◊î◊ñ◊û◊ü. ◊§◊©◊ï◊ò ◊ô◊ï◊™◊® ◊û◊ì◊ô ◊©◊ê◊ú◊ï◊™ ◊ï◊°◊¢◊ô◊§◊ô◊ù ◊ú◊û◊ë◊ó◊ü ◊©◊ú ◊©◊¢◊™◊ô◊ô◊ù ◊õ◊ö ◊©◊î◊®◊ë◊î ◊ê◊†◊©◊ô◊ù ◊ê◊§◊ô◊ú◊ï ◊ú◊ê ◊î◊°◊§◊ô◊ß◊ï ◊ó◊ú◊ß ◊û◊î◊û◊ë◊ó◊ü. ◊ê◊§◊©◊® ◊ú◊î◊ë◊ô◊ê ◊ì◊£ ◊†◊ï◊°◊ó◊ê◊ï◊™ ◊ê◊ô◊©◊ô ◊©◊û◊õ◊ô◊†◊ô◊ù ◊ú◊ë◊ì ◊ê◊ñ ◊í◊ù ◊î◊î◊õ◊†◊î ◊©◊ú◊ï ◊¢◊ï◊ñ◊®◊™ ◊ú◊¢◊ë◊ï◊® ◊¢◊ú ◊î◊ó◊ï◊û◊®.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ê◊ó◊ú◊î ◊ß◊ï◊®◊° ◊ú◊°◊û◊°◊ò◊® ◊®◊ê◊©◊ï◊ü. ◊™◊¢◊ë◊®◊ï ◊¢◊ú ◊î◊û◊¶◊í◊ï◊™ ◊ú◊§◊†◊ô ◊î◊û◊ë◊ó◊ü ◊ï◊¢◊ú ◊©◊ê◊ú◊ï◊™ ◊û◊û◊ë◊ó◊†◊ô ◊¢◊ë◊® ◊ï◊™◊ï◊°◊ô◊§◊ï ◊ú◊ì◊£ ◊†◊ï◊°◊ó◊ê◊ï◊™ ◊õ◊ú ◊ì◊ë◊® ◊©◊†◊®◊ê◊î ◊ú◊õ◊ù ◊®◊ú◊ï◊ï◊†◊ò◊ô ◊ï◊™◊î◊ô◊ï ◊ë◊°◊ì◊® :)
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊†◊ï◊õ◊ó◊ï◊™ ◊ó◊ï◊ë◊î

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊ú◊ê ◊î◊õ◊®◊ó◊ô ◊ú◊î◊ë◊†◊™ ◊î◊ó◊ï◊û◊®

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊™◊ï◊ê◊ù ◊ú◊®◊û◊™ ◊î◊û◊ë◊ó◊ü

◊î◊û◊ë◊ó◊ü: ◊õ◊ú ◊î◊ó◊ï◊û◊® ◊î◊†◊ú◊û◊ì ◊ë◊™◊®◊í◊ï◊ú◊ô◊ù ◊î◊¶◊ú◊ô◊ó ◊ú◊î◊õ◊†◊° ◊ú◊û◊ë◊ó◊ü ◊©◊ú ◊©◊¢◊™◊ô◊ô◊ù ◊ê◊ñ ◊õ◊ì◊ê◊ô ◊ú◊¢◊ß◊ï◊ë ◊ê◊ó◊®◊ô ◊û◊î ◊ú◊ï◊û◊ì◊ô◊ù ◊ë◊û◊î◊ú◊ö ◊î◊°◊û◊°◊ò◊®

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ê◊§◊©◊®◊ô ◊ï◊°◊ë◊ô◊®, ◊†◊ô◊™◊ü ◊ú◊ú◊û◊ï◊ì ◊í◊ù ◊ë◊ú◊ô ◊ú◊î◊í◊ô◊¢ ◊ú◊î◊®◊¶◊ê◊ï◊™ ◊ê◊ï ◊ú◊™◊®◊í◊ï◊ú◊ô◊ù ◊§◊®◊ò ◊ú◊¢◊†◊ô◊ô◊ü ◊î◊†◊ï◊õ◊ó◊ï◊™ ◊î◊ó◊ï◊ë◊î
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£ (◊ê◊ô◊ô◊®◊ï◊ü ◊û◊ü) ◊ê◊ë◊®◊î◊û◊ô
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊™◊™◊õ◊ï◊†◊†◊ï ◊ú◊ñ◊î ◊©◊®◊ï◊ë ◊î◊ñ◊û◊ü ◊ê◊°◊£ ◊û◊ì◊ë◊® ◊¢◊ú ◊¢◊¶◊û◊ï ◊ï◊¢◊ú ◊õ◊û◊î ◊î◊ï◊ê ◊™◊ï◊™◊ó. ◊ê◊™ ◊î◊ó◊ï◊û◊® ◊î◊ï◊ê ◊ú◊®◊ï◊ë ◊û◊¢◊ë◊ô◊® ◊û◊î◊û◊¶◊í◊™. ◊ô◊© ◊©◊ê◊ú◊™ ◊©◊ë◊ï◊¢ ◊õ◊ú ◊©◊ë◊ï◊¢ ◊õ◊õ◊î ◊©◊ú◊§◊¢◊û◊ô◊ù ◊ñ◊î ◊õ◊ü ◊û◊™◊§◊™◊ó ◊ú◊©◊ô◊ó ◊†◊ó◊û◊ì, ◊™◊ú◊ï◊ô ◊õ◊û◊î ◊î◊ï◊ê ◊†◊ï◊™◊ü ◊ú◊ñ◊î ◊û◊ß◊ï◊ù ◊ï◊ñ◊û◊ü. ◊î◊†◊ï◊õ◊ó◊ï◊™ ◊ó◊ï◊ë◊î ◊ê◊ñ ◊ê◊ô◊ü ◊ô◊ï◊™◊® ◊û◊ì◊ô ◊ë◊®◊ô◊®◊î. ◊ú◊†◊ï ◊ñ◊î ◊î◊ô◊î ◊§◊™◊ô◊ó◊™ ◊©◊ë◊ï◊¢ ◊ó◊û◊ï◊ì◊î ◊ï◊ú◊ê ◊û◊¢◊ë◊®. ◊™◊©◊™◊ì◊ú◊ï ◊õ◊ü ◊ú◊î◊©◊™◊™◊£ ◊ë◊©◊ë◊ô◊ú ◊î◊¢◊ï◊ì ◊õ◊û◊î ◊†◊ß◊ï◊ì◊ï◊™ ◊î◊ê◊ú◊î ◊ë◊°◊ï◊£ ◊î◊ß◊ï◊®◊° (◊ï◊©◊ô◊ñ◊õ◊ï◊® ◊ê◊™ ◊î◊©◊ù ◊©◊ú◊õ◊ù), ◊ï◊™◊¢◊©◊ï ◊ê◊™ ◊î◊û◊¶◊í◊™ 5 ◊ì◊ß' ◊©◊û◊ï◊°◊ô◊§◊î ◊†◊ß◊ï◊ì◊ï◊™ ◊ó◊ô◊†◊ù ◊ú◊¶◊ô◊ï◊ü ◊î◊°◊ï◊§◊ô.

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î ◊ô◊®◊ï◊ü
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊†◊ó◊û◊ô◊î ◊ú◊ê ◊ë◊ê◊û◊™ ◊ô◊ï◊ì◊¢ ◊ú◊ú◊û◊ì, ◊¢◊ì ◊î◊ô◊ï◊ù ◊ú◊ê ◊ë◊®◊ï◊® ◊ú◊ô ◊û◊î ◊î◊î◊©◊õ◊ú◊î/◊î◊û◊ß◊¶◊ï◊¢ ◊©◊ú◊ï. ◊õ◊ì◊ô ◊ú◊û◊ú◊ê ◊ê◊™ ◊õ◊ú ◊î◊©◊¢◊î ◊©◊ô◊© ◊ú◊ï ◊î◊ï◊ê ◊§◊©◊ï◊ò ◊§◊ï◊™◊® ◊ê◊™ ◊î◊©◊ê◊ú◊î ◊û◊î◊û◊¶◊í◊™ ◊¢◊ú ◊î◊ú◊ï◊ó (◊û◊î ◊©◊û◊ï◊§◊ô◊¢ ◊ë◊©◊ß◊ï◊§◊ô◊™ ◊î◊ë◊ê◊î ◊ê◊©◊õ◊®◊î), ◊ï◊í◊ù ◊ô◊© ◊ë◊û◊ï◊ì◊ú ◊§◊™◊®◊ï◊†◊ï◊™ ◊ú◊™◊®◊í◊ï◊ú◊ô◊ù ◊©◊ú◊ï ◊õ◊ß◊ï◊ë◊• ◊ï◊ú◊ê ◊®◊ß ◊õ◊û◊¶◊í◊™. ◊î◊™◊®◊í◊ï◊ú◊ô◊ù ◊ê◊¶◊ú◊†◊ï ◊ú◊ê ◊î◊ô◊ï ◊ó◊ï◊ë◊î ◊ï◊í◊ù ◊î◊ù ◊™◊ß◊¢◊ï ◊ê◊ï◊™◊ù ◊ë6 ◊ë◊¢◊®◊ë ◊ê◊ñ ◊ê◊£ ◊ê◊ó◊ì ◊ú◊ê ◊î◊ú◊ö. ◊ë◊ß◊ô◊¶◊ï◊®- ◊û◊ô◊ï◊™◊®.

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊õ◊ú ◊©◊ë◊ï◊¢, ◊û◊û◊ï◊ó◊ñ◊®◊ô◊ù ◊®◊¶◊ó. ◊ê◊§◊©◊® ◊ú◊®◊§◊®◊†◊° ◊ê◊ë◊ú ◊ê◊§◊©◊® ◊ë◊õ◊ô◊£ ◊í◊ù ◊ú◊§◊™◊ï◊® ◊ê◊ï◊™◊ù ◊ú◊ë◊ì (◊ê◊ï ◊ë◊ñ◊ï◊í◊ï◊™) ◊õ◊ì◊ô ◊ú◊ï◊ï◊ì◊ê ◊î◊ë◊†◊î ◊©◊ú ◊î◊ó◊ï◊û◊® ◊î◊©◊ë◊ï◊¢ (◊õ◊ê◊û◊ï◊®- ◊õ◊ú ◊©◊ë◊ï◊¢ ◊†◊ï◊©◊ê ◊ê◊ó◊® ◊ú◊í◊û◊®◊ô ◊ë◊ß◊ï◊®◊°). ◊™◊™◊õ◊ï◊†◊†◊ï ◊†◊§◊©◊ô◊™ ◊ú◊ñ◊î ◊©◊î◊ï◊ê ◊ú◊ê ◊û◊ó◊ñ◊ô◊® ◊¶◊ô◊ï◊†◊ô◊ù ◊¢◊ú ◊î◊û◊ò◊ú◊ï◊™ ◊¢◊ì ◊î◊û◊ë◊ó◊ü (◊™◊†◊°◊ï ◊ú◊î◊¶◊ô◊ß ◊ú◊ï ◊ï◊ú◊ê◊°◊£ ◊¢◊ú ◊ñ◊î ◊ë◊û◊î◊ú◊ö ◊î◊°◊û◊°◊ò◊® ◊ô◊ï◊™◊®).

◊î◊û◊ë◊ó◊ü: ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊ê◊û◊ï◊® ◊ú◊î◊ô◊ï◊™ ◊û◊ë◊ó◊ü ◊û◊™◊†◊î ◊ê◊ù ◊ú◊ï◊û◊ì◊ô◊ù ◊ê◊ú◊ô◊ï ◊ë◊ê◊û◊™. ◊ú◊ê ◊ú◊ñ◊ú◊ñ◊ú ◊õ◊ô ◊ñ◊î ◊ë◊ê◊°◊î ◊ú◊ß◊ë◊ú ◊¶◊ô◊ï◊ü ◊í◊®◊ï◊¢ (◊ê◊ï ◊ú◊ê ◊ú◊¢◊ë◊ï◊®) ◊û◊ë◊ó◊ü ◊©◊ô◊õ◊ï◊ú ◊ú◊ú◊õ◊™ ◊°◊ë◊ë◊î ◊ú◊í◊û◊®◊ô, ◊ú◊¢◊ï◊û◊™ ◊î◊©◊ê◊® ◊ë◊°◊û◊°◊ò◊® ◊î◊ñ◊î. ◊û◊î ◊©◊õ◊ü- ◊ê◊¶◊ú◊†◊ï ◊î◊ù ◊ô◊¶◊ê◊ï ◊û◊†◊ô◊ê◊ß◊ô◊ù ◊ß◊¶◊™. ◊î◊ù ◊†◊ô◊°◊ï ◊ú◊ì◊ó◊ï◊° ◊ê◊™ ◊õ◊ú ◊î◊ó◊ï◊û◊® ◊©◊ú ◊î◊ß◊ï◊®◊° ◊ë◊û◊ë◊ó◊ü ◊ê◊ó◊ì ◊õ◊õ◊î ◊©◊õ◊ú ◊°◊¢◊ô◊£ ◊ú◊ß◊ó ◊†◊¶◊ó ◊ï◊ê◊†◊©◊ô◊ù ◊ú◊ê ◊î◊°◊§◊ô◊ß◊ï ◊ú◊¢◊†◊ï◊™ ◊¢◊ú ◊õ◊ú ◊î◊û◊ë◊ó◊ü (◊©◊¢◊™◊ô◊ô◊ù). ◊ë◊û◊ë◊ó◊†◊ô ◊¢◊ë◊® ◊ñ◊î ◊ú◊ê ◊î◊ô◊î ◊õ◊õ◊î (◊ï◊í◊ù ◊î◊ù ◊ú◊ê ◊î◊°◊õ◊ô◊û◊ï ◊ú◊§◊®◊°◊ù ◊ê◊™ ◊î◊û◊ë◊ó◊ü ◊©◊ú ◊©◊†◊î ◊©◊¢◊ë◊®◊î ◊ï◊î◊§◊™◊®◊ï◊ü). ◊ï◊í◊ù ◊î◊ô◊ï ◊ë◊¢◊ô◊ï◊™ ◊ë◊†◊ô◊°◊ï◊ó◊ô◊ù. ◊°◊î◊õ ◊õ◊ú ◊î◊ë◊¢◊ô◊ï◊™ ◊î◊™◊†◊ß◊ñ◊ï ◊ú5 ◊†◊ß ◊§◊ß◊ò◊ï◊® ◊©◊ò◊®◊ó◊ï ◊ú◊™◊™, ◊û◊ß◊ï◊ï◊î ◊©◊ô◊î◊ô◊ï ◊ô◊ï◊™◊® ◊î◊ï◊í◊†◊ô◊ù ◊ú◊î◊ë◊ê.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊î◊ó◊ï◊û◊® ◊¢◊¶◊û◊ï ◊ú◊ê ◊û◊ê◊ï◊ì ◊ß◊©◊î, ◊ê◊ë◊ú ◊õ◊ü ◊û◊ï◊®◊õ◊ë ◊û◊î◊®◊ë◊î ◊†◊ï◊©◊ê◊ô◊ù ◊©◊õ◊ú ◊ê◊ó◊ì ◊¢◊ï◊û◊ì ◊ë◊§◊†◊ô ◊¢◊¶◊û◊ï ◊ï◊õ◊ì◊ê◊ô ◊ú◊©◊ô◊ù ◊ú◊ë ◊ú◊õ◊ï◊ú◊ù. ◊°◊î◊õ ◊ß◊ï◊®◊° ◊ó◊ë◊ô◊ë, ◊†◊ï◊™◊ü ◊°◊ô◊§◊™◊ó (◊ß◊ú◊ï◊© ◊ê◊ë◊ú ◊†◊ï◊™◊ü) ◊ú◊™◊ï◊ê◊®. ◊™◊™◊¢◊ß◊©◊ï ◊¢◊ú ◊ì◊ë◊®◊ô◊ù ◊©◊û◊í◊ô◊¢ ◊ú◊õ◊ù ◊ú◊ß◊ë◊ú ◊ï◊™◊ñ◊®◊û◊ï ◊¢◊ù ◊î◊î◊†◊§◊¶◊ï◊™ ◊©◊ú◊î◊ù (◊°◊ô◊ï◊® ◊û◊ô◊ï◊™◊®, ◊°◊ì◊†◊™ ◊î◊ì◊§◊°◊™ ◊™◊ú◊™ ◊û◊ô◊û◊ì, ◊¢◊ë◊ï◊ì◊î ◊¢◊ú ◊î◊™◊ï◊õ◊†◊î ◊©◊ê◊°◊£ ◊î◊û◊¶◊ô◊ê ◊ï◊õ◊ï')
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊§◊®◊ï◊§◊°◊ï◊® ◊ê◊°◊£ ◊ê◊ë◊®◊î◊û◊ô
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊ê◊°◊£ ◊û◊ì◊ë◊® ◊¢◊ú ◊¢◊¶◊û◊ï ◊ë◊ß◊ò◊¢ ◊©◊õ◊ë◊® ◊í◊ï◊®◊ù ◊ê◊ô ◊†◊ï◊ó◊ï◊™

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î ◊ô◊®◊ï◊ü
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù:
◊ó◊ó◊ó◊ó◊ó
◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™:
◊™◊ß◊ï◊¢ ◊ë◊ô◊ü ◊õ◊ú ◊©◊ê◊® ◊î◊û◊ò◊ú◊ï◊™ ◊î◊ê◊û◊ô◊™◊ô◊ï◊™, ◊ú◊ê ◊õ◊ñ◊î ◊ß◊®◊ô◊ò◊ô ◊ú◊î◊™◊¢◊û◊ß ◊ë◊û◊î◊ú◊ö ◊î◊°◊û◊°◊ò◊® ◊ê◊§◊©◊® ◊ú◊î◊¢◊ñ◊® ◊ë◊®◊§◊®◊†◊°◊ô◊ù.
◊î◊û◊ë◊ó◊ü:
◊ú◊ß◊ó◊™ 4 ◊ô◊û◊ô◊ù ◊ú◊§◊†◊ô ◊ú◊ó◊®◊ï◊© ◊®◊ß ◊¢◊ú ◊ñ◊î ◊ï◊ê◊ñ ◊û◊ï◊¶◊ô◊ê◊ô◊ù ◊¶◊ô◊ï◊ü ◊ò◊ï◊ë
◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ß◊ï◊®◊° ◊û◊ë◊ê◊° ◊ê◊ô◊ü ◊û◊î ◊ú◊¢◊©◊ï◊™. ◊î◊†◊ï◊©◊ê◊ô◊ù ◊¢◊¶◊û◊ù ◊°◊ï◊§◊® ◊û◊¢◊†◊ô◊†◊ô◊ù ◊ï◊ë◊î◊™◊ó◊ú◊î ◊©◊ú ◊õ◊ú ◊†◊ï◊©◊ê ◊ô◊© ◊§◊ï◊ò◊†◊¶◊ô◊ê◊ú ◊©◊ë◊ê◊û◊™ ◊ô◊î◊ô◊î ◊û◊¢◊†◊ô◊ô◊ü, ◊ê◊ë◊ú ... ◊ê◊°◊£ ◊ï◊†◊ó◊û◊ô◊î... ◊ñ◊î ◊ú◊ê ◊ß◊ï◊®◊° ◊ê◊û◊ô◊™◊ô ◊ï◊™◊†◊°◊ï ◊ú◊î◊™◊õ◊ï◊†◊ü ◊†◊§◊©◊ô◊™ ◊ú◊õ◊ú ◊î◊ì◊ë◊®◊ô◊ù ◊©◊î◊ù ◊™◊ï◊ß◊¢◊ô◊ù ◊ë◊û◊î◊ú◊ö ◊î◊°◊û◊ò◊°◊®(◊ó◊©◊ë◊©◊ë◊™, ◊°◊ì◊†◊î,◊§◊®◊ï◊ô◊ß◊ò◊ï◊ü)
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊§◊®◊ï◊§' ◊û◊†◊õ"◊ú ◊ß◊¶◊ô◊ü ◊ë◊ì◊ô◊û◊ï◊° ◊ï◊í◊ô◊ë◊ï◊® ◊î◊û◊ï◊ú◊ì◊™ ◊ê◊°◊£ ◊ê◊ë◊®◊î◊û◊ô
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊™◊õ◊ú◊°, ◊ó◊ï◊• ◊û◊ñ◊î ◊©◊î◊ï◊ê ◊û◊™◊†◊©◊ê, ◊ñ◊î ◊ì◊ô◊ô ◊ê◊ó◊ú◊î, ◊¶◊®◊ô◊ö ◊ú◊î◊ß◊©◊ô◊ë ◊õ◊ô ◊ô◊© ◊©◊ê◊ú◊î ◊¢◊ú ◊û◊î ◊©◊î◊ï◊ê ◊û◊ì◊ë◊® ◊ë◊û◊ë◊ó◊ü. ◊ú◊§◊¢◊û◊ô◊ù ◊ô◊© ◊î◊®◊¶◊ê◊ï◊™ ◊ê◊ï◊®◊ó ◊©◊ô◊õ◊ï◊ú◊ï◊™ ◊ú◊î◊ô◊ï◊™ ◊ê◊ó◊ú◊î, ◊î◊ì◊ë◊®◊ô◊ù ◊ë◊ê◊û◊¶◊¢ (◊™◊õ"◊ü ◊ï◊ó◊©◊ë◊©◊ë◊™) ◊ó◊®◊ò◊ê ◊ú◊í◊û◊®◊ô. ◊ë◊°◊ï◊£ ◊ó◊û◊ï◊ì, ◊ê◊û◊ï◊® ◊ú◊î◊¢◊ú◊ï◊™ ◊¶◊ô◊ï◊ü ◊ë◊°◊û◊°◊ò◊® ◊ê'

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î "◊ñ◊î ◊ê◊ô◊†◊ò◊ï◊ê◊ô◊ò◊ô◊ë◊ô" ◊ô◊®◊ï◊ü
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊ó◊û◊ï◊ì, ◊ë◊°◊ï◊£ ◊ñ◊î ◊î◊ó◊ú◊ß ◊î◊ê◊û◊ô◊™◊ô ◊©◊ú ◊î◊¶◊ô◊ï◊ü, ◊û◊ï◊û◊ú◊• ◊ú◊®◊ê◊ï◊™ ◊ê◊™ ◊î◊û◊¶◊í◊ï◊™ ◊ï◊ê◊ù ◊ú◊ê ◊î◊ë◊†◊™◊ù ◊ú◊ë◊ï◊ê.

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊ó◊®◊ò◊ê, ◊§◊¢◊ù ◊ë◊©◊ë◊ï◊¢ ◊©◊¢◊î ◊í◊í ◊ë◊ñ◊ï◊í◊ï◊™

◊î◊û◊ë◊ó◊ü: ◊î◊ï◊í◊ü, ◊õ◊ê◊ô◊ú◊ï ◊î◊ó◊ï◊û◊® ◊ú◊§◊¢◊û◊ô◊ù ◊û◊û◊© ◊ú◊ê ◊û◊®◊í◊ô◊© ◊ó◊©◊ï◊ë ◊ê◊ë◊ú ◊¶◊®◊ô◊ö ◊ú◊î◊ë◊ô◊ü ◊ê◊™ ◊û◊î ◊©◊®◊ï◊¶◊ô◊ù ◊û◊û◊ö.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ê◊ô◊ü ◊ë◊ó◊ô◊®◊î ◊ê◊ñ ◊ë◊ï◊ê◊ï, ◊ë◊®◊ê◊©◊ï◊ü ◊ñ◊ï ◊ê◊ó◊ú◊î ◊ì◊®◊ö ◊ú◊§◊™◊ï◊ó ◊©◊ë◊ï◊¢ ◊ë◊õ◊ô◊£
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£ ◊ê◊ë◊®◊î◊û◊ô
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊†◊ï◊õ◊ó◊ï◊™ ◊ó◊ï◊ë◊î. ◊ë◊í◊ì◊ï◊ú ◊†◊ó◊û◊ì ◊ú◊§◊™◊ï◊ó ◊õ◊õ◊î ◊ê◊™ ◊ô◊ï◊ù ◊®◊ê◊©◊ï◊ü ◊ï◊ú◊ê ◊ë◊ß◊ï◊®◊°◊ô◊ù ◊©◊ú ◊û◊™◊û◊ò◊ô◊ß◊î. ◊ê◊°◊£ ◊û◊™ ◊¢◊ú ◊¢◊¶◊û◊ï ◊ï◊ë◊¢◊ô◊ß◊® ◊û◊ì◊ë◊® ◊¢◊ú◊ô◊ï ◊ï◊¢◊ú ◊î◊ó◊ë◊®◊î ◊©◊ú◊ï ◊ë◊î◊®◊¶◊ê◊ï◊™.

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î ◊ô◊®◊ï◊ü
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊û◊§◊°◊ô◊ß◊ô◊ù ◊ú◊ú◊õ◊™ ◊ê◊ó◊®◊ô ◊î◊™◊®◊í◊ï◊ú◊ô◊ù ◊î◊®◊ê◊©◊ï◊†◊ô◊ù, ◊ú◊ê ◊ó◊ï◊ë◊î

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊î◊¢◊™◊ß ◊î◊ì◊ë◊ß ◊û◊î◊û◊¶◊í◊™ ◊©◊ú ◊î◊™◊®◊í◊ï◊ú, ◊ê◊ë◊ú ◊û◊¢◊ô◊ß ◊©◊ô◊© ◊©◊ô◊¢◊ï◊®◊ô ◊ë◊ô◊™ ◊ë◊ñ◊î ◊§◊¢◊ù ◊ë◊©◊ë◊ï◊¢ ◊õ◊ô ◊ú◊§◊¢◊û◊ô◊ù ◊ñ◊î ◊í◊ï◊ñ◊ú ◊ñ◊û◊ü.

◊î◊û◊ë◊ó◊ü: ◊î◊û◊ë◊ó◊ü ◊î◊©◊†◊î ◊î◊ô◊î ◊ê◊®◊ï◊ö ◊ï◊ß◊©◊î ◊û◊ê◊ï◊ì ◊ë◊ô◊ó◊° ◊ú◊©◊†◊ô◊ù ◊ß◊ï◊ì◊û◊ï◊™. ◊ú◊ê ◊î◊ô◊î ◊î◊ï◊í◊ü.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ß◊ï◊®◊° ◊ó◊û◊ï◊ì, ◊ú◊ê ◊¶◊®◊ô◊ö ◊ú◊î◊©◊ß◊ô◊¢ ◊ô◊ï◊™◊® ◊û◊ô◊ì◊ô ◊ó◊ï◊• ◊û◊î◊©◊ô◊¢◊ï◊®◊ô ◊ë◊ô◊™ ◊õ◊ú ◊©◊ë◊ï◊¢.
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™:
◊û◊ú◊ê ◊§◊ï◊ò◊†◊¶◊ô◊ê◊ú ◊ú◊î◊ô◊ï◊™ ◊û◊¢◊†◊ô◊ô◊ü ◊ï◊õ◊ô◊ô◊§◊ô ◊ê◊ë◊ú ◊ê◊°◊£ ◊õ◊ú◊õ◊ö ◊ê◊ï◊î◊ë ◊ê◊™ ◊î◊™◊ó◊™ ◊©◊ú ◊¢◊¶◊û◊ï ◊©◊ñ◊î ◊ì◊ô ◊î◊ï◊®◊° ◊ê◊™ ◊î◊©◊ô◊¢◊ï◊®. ◊î◊ô◊î ◊ô◊õ◊ï◊ú ◊ú◊î◊ô◊ï◊™ ◊ê◊ó◊ú◊î ◊î◊ñ◊ì◊û◊†◊ï◊™ ◊ú◊§◊™◊ó ◊©◊ô◊ó◊î ◊û◊¢◊†◊ô◊ô◊†◊™ ◊ë◊õ◊ú ◊û◊†◊ô ◊†◊ï◊©◊ê◊ô◊ù (◊õ◊ú ◊©◊ë◊ï◊¢ ◊ô◊© ◊©◊ê◊ú◊™ ◊©◊ë◊ï◊¢ ◊ï◊§◊ï◊™◊ó◊ô◊ù ◊¢◊ú◊ô◊î ◊ì◊ô◊ï◊ü) ◊ê◊ë◊ú ◊û◊®◊í◊ô◊© ◊©◊ê◊ô◊ü ◊û◊ß◊ï◊ù ◊ê◊û◊ô◊™◊ô ◊ú◊©◊™◊£ ◊ë◊õ◊ú◊ï◊ù. ◊î◊†◊ï◊õ◊ó◊ï◊™ ◊î◊ô◊ê ◊ó◊ï◊ë◊î ◊ê◊ë◊ú ◊ñ◊î ◊ó◊®◊ò◊ê ◊ñ◊î ◊®◊ß ◊õ◊ô ◊ë◊°◊ï◊£ ◊î◊°◊û◊°◊ò◊® ◊ê◊°◊£ ◊û◊ë◊ô◊ê ◊¶◊ô◊ï◊ü ◊¢◊ú ◊î◊î◊©◊™◊™◊§◊ï◊™ ◊ë◊õ◊ô◊™◊î. ◊î◊ô◊ô◊™◊ô ◊û◊û◊ú◊ô◊¶◊î ◊§◊©◊ï◊ò ◊ú◊ì◊ë◊® ◊û◊°◊§◊ô◊ß ◊õ◊ì◊ô ◊©◊ô◊ñ◊õ◊ï◊® ◊ê◊™ ◊î◊©◊ù ◊©◊ú◊õ◊ù ◊ï◊ê◊ñ ◊û◊ß◊ë◊ú◊ô◊ù ◊û◊ê◊î
◊ë◊†◊ï◊°◊£ ◊ô◊© ◊û◊¶◊í◊™ ◊©◊ú◊ê ◊ó◊ô◊ô◊ë◊ô◊ù ◊ú◊¢◊©◊ï◊™ ◊ê◊ë◊ú ◊û◊¢◊ú◊î ◊ê◊™ ◊î◊¶◊ô◊ï◊ü ◊î◊°◊ï◊§◊ô ◊ê◊ñ ◊ú◊û◊î ◊ú◊ê

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù:
◊ú◊ê ◊î◊ú◊õ◊™◊ô ◊î◊ô◊î ◊û◊ô◊ï◊™◊®

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™:
◊ô◊© ◊õ◊ú ◊©◊ë◊ï◊¢, ◊ï◊ë◊°◊ï◊£ ◊ñ◊î 15% ◊û◊î◊¶◊ô◊ï◊ü. ◊û◊û◊ó◊ï◊ñ◊®◊ô◊ù ◊ú◊í◊û◊®◊ô, ◊ê◊§◊©◊® ◊ú◊¢◊©◊ï◊™ 99 ◊ê◊ó◊ï◊ñ ◊¢◊ù ◊®◊§◊®◊†◊°◊ô◊ù ◊ê◊ë◊ú ◊í◊ù ◊û◊û◊© ◊ß◊ú ◊ú◊¢◊©◊ï◊™ ◊ú◊ë◊ì. ◊î◊ù ◊™◊õ◊ú◊° ◊ú◊î◊í◊©◊î ◊ë◊ñ◊ï◊í◊ï◊™ ◊ê◊ë◊ú ◊ú◊ê ◊ó◊ï◊ë◊î. ◊ú◊†◊ï ◊î◊°◊û◊°◊ò◊® ◊î◊ô◊ô◊™◊î ◊ë◊ï◊ì◊ß◊™ (◊ú◊ê ◊†◊ó◊û◊ô◊î) ◊ê◊ñ ◊ì◊ï◊ï◊ß◊ê ◊î◊ó◊ñ◊ô◊®◊ï ◊¶◊ô◊ï◊†◊ô◊ù ◊ì◊ô ◊û◊î◊®.
◊î◊©◊ô◊¢◊ï◊®◊ô ◊ë◊ô◊™ ◊î◊û◊¢◊¶◊ë◊ô◊ù ◊ë◊ê◊û◊™ ◊ñ◊î ◊î◊¢◊ë◊ï◊ì◊î ◊¢◊ú ◊î◊ó◊©◊ë◊©◊ë◊™ ◊ï◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ï◊ü. ◊î◊ô◊ô◊™◊ô ◊û◊û◊ú◊ô◊¶◊î ◊ú◊î◊™◊ó◊ô◊ú ◊ê◊™ ◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ï◊ü ◊õ◊û◊î ◊©◊ô◊ï◊™◊® ◊û◊ï◊ß◊ì◊ù (◊ê◊§◊ô◊ú◊ï ◊©◊ë◊ï◊¢ 3-4) ◊ï◊ú◊ê ◊ú◊î◊©◊ê◊ô◊® ◊ê◊™ ◊ñ◊î ◊ú◊°◊ï◊£ ◊î◊°◊û◊°◊ò◊® ◊õ◊©◊õ◊ë◊® ◊ô◊© ◊¢◊ï◊û◊° ◊ï◊û◊ë◊ó◊†◊ô◊ù ◊¢◊ú ◊î◊®◊ê◊©

◊î◊û◊ë◊ó◊ü:
◊î◊ô◊î ◊¢◊û◊ï◊° ◊û◊û◊© ◊ï◊ú◊ê ◊ê◊§◊©◊®◊ô ◊ú◊°◊ô◊ô◊ù ◊ë◊©◊¢◊™◊ô◊ô◊ù, ◊†◊™◊†◊ï ◊§◊ß◊ò◊ï◊® ◊©◊ú 4 ◊†◊ß◊ï◊ì◊ï◊™ ◊©◊ë◊ê◊û◊™ ◊î◊ô◊î ◊ë◊ì◊ô◊ó◊î ◊ô◊ó◊°◊ô◊™ ◊ú◊û◊û◊ï◊¶◊¢. ◊ë◊™◊õ◊ú◊° ◊î◊ó◊ï◊û◊® ◊ú◊ê ◊û◊°◊ï◊ë◊ö (◊ó◊ï◊• ◊û◊î◊©◊ë◊ï◊¢ ◊©◊ú ◊î◊°◊™◊ë◊®◊ï◊™ ◊©◊î◊ï◊ê ◊ó◊®◊ê ◊ï◊™◊û◊ô◊ì ◊û◊ï◊§◊ô◊¢ ◊ë◊û◊ë◊ó◊ü) ◊ê◊ë◊ú ◊§◊©◊ï◊ò ◊ô◊© ◊û◊ú◊ê ◊ó◊ï◊û◊®, ◊ï◊û◊ï◊™◊® ◊®◊ß ◊ì◊£ ◊†◊ï◊°◊ó◊ê◊ï◊™ ◊ê◊ó◊ì ◊ê◊ñ ◊¶◊®◊ô◊ö ◊ú◊õ◊™◊ï◊ë ◊û◊û◊© ◊ß◊ò◊ü ◊õ◊ô ◊ë◊ê◊û◊™ ◊ô◊© ◊û◊ú◊ê ◊û◊ô◊ì◊¢ ◊©◊¶◊®◊ô◊ö ◊ú◊û◊ë◊ó◊ü.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ß◊ï◊®◊° ◊ó◊ï◊ë◊î ◊ó◊û◊ï◊ì, ◊ï◊ê◊ó◊ú◊î ◊î◊ñ◊ì◊û◊†◊ï◊™ ◊ú◊î◊õ◊ô◊® ◊ê◊™ ◊î◊ê◊†◊©◊ô◊ù ◊©◊ê◊ô◊™◊õ◊ù ◊ë◊™◊ï◊ê◊®. ◊î◊û◊ë◊ó◊ü ◊ß◊¶◊™ ◊û◊ë◊ê◊°
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°
---
◊©◊ù ◊î◊û◊®◊¶◊î: ◊ê◊°◊£ ◊ê◊ë◊®◊î◊û◊ô
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊î◊®◊¶◊ê◊ï◊™: ◊†◊ï◊õ◊ó◊ï◊™ ◊ó◊ï◊ë◊î ◊ê◊ë◊ú ◊†◊ó◊û◊ì ◊ú◊§◊™◊ï◊ó ◊ê◊™ ◊î◊©◊ë◊ï◊¢ ◊ë◊û◊©◊î◊ï ◊©◊î◊ï◊ê ◊ú◊ê ◊û◊™◊û◊ò◊ô◊ß◊î. ◊ê◊°◊£ ◊ë◊¢◊ô◊ß◊® ◊ê◊ï◊î◊ë ◊ú◊ì◊ë◊® ◊¢◊ú ◊¢◊¶◊û◊ï ◊ï◊¢◊ú ◊î◊ó◊ë◊®◊î ◊©◊ú◊ï ◊ï◊û◊õ◊†◊ô◊° ◊ê◊™ ◊ñ◊î ◊ú◊ì◊ô◊ï◊ü ◊ë◊õ◊ú ◊î◊ñ◊ì◊û◊†◊ï◊™. ◊û◊¢◊ë◊® ◊ú◊ñ◊î ◊î◊î◊®◊¶◊ê◊ï◊™ ◊†◊ó◊û◊ì◊ï◊™, ◊ú◊ê ◊û◊¢◊ë◊®. ◊î◊®◊¶◊ê◊ï◊™ ◊î◊ê◊ï◊®◊ó ◊©◊ú ◊†◊ï◊¢◊ù ◊î◊ô◊ï ◊ê◊ó◊ú◊î ◊ú◊í◊û◊®◊ô.

◊©◊ù ◊î◊û◊™◊®◊í◊ú/◊™: ◊†◊ó◊û◊ô◊î ◊ô◊®◊ï◊ü
◊ó◊ï◊ï◊™ ◊ì◊¢◊™ - ◊™◊®◊í◊ï◊ú◊ô◊ù: ◊ê◊ó◊®◊ô ◊©◊†◊ô ◊™◊®◊í◊ï◊ú◊ô◊ù ◊î◊ë◊†◊ï ◊©◊î◊ù ◊û◊ô◊ï◊™◊®◊ô◊ù ◊ï◊î◊§◊°◊ß◊†◊ï ◊ú◊ú◊õ◊™.

◊©◊¢◊ï◊®◊ô ◊î◊ë◊ô◊™: ◊î◊®◊ï◊ë ◊ñ◊î ◊î◊¢◊™◊ß ◊î◊ì◊ë◊ß ◊û◊î◊û◊¶◊í◊™ ◊©◊ú ◊î◊™◊®◊í◊ï◊ú ◊®◊ß ◊©◊û◊©◊†◊ô◊ù ◊†◊™◊ï◊†◊ô◊ù. ◊ê◊ë◊ú ◊©◊ñ◊î ◊ú◊ê ◊õ◊û◊ï ◊ë◊û◊¶◊í◊™ ◊ñ◊î ◊©◊ë◊ô◊®◊™ ◊®◊ê◊©. ◊ô◊© ◊í◊ô◊ú◊ô◊ï◊ü ◊õ◊ú ◊©◊ë◊ï◊¢ ◊©◊ñ◊î ◊û◊¢◊ô◊ß ◊õ◊ô ◊ú◊§◊¢◊û◊ô◊ù ◊ô◊ï◊¶◊ê ◊©◊û◊ß◊ì◊ô◊©◊ô◊ù ◊î◊®◊ë◊î ◊ñ◊û◊ü ◊ú◊ß◊ï◊®◊° ◊ô◊ó◊°◊ô◊™ ◊ß◊ò◊ü ◊ô◊ï◊™◊® ◊ë◊ô◊ó◊° ◊ú◊©◊ê◊®.

◊î◊û◊ë◊ó◊ü: ◊î◊û◊ë◊ó◊ü ◊î◊§◊¢◊ù ◊î◊ô◊î ◊ê◊®◊ï◊ö ◊û◊û◊© ◊ë◊ô◊ó◊° ◊ú◊©◊†◊ô◊ù ◊ß◊ï◊ì◊û◊ï◊™ ◊ï◊ú◊ì◊¢◊™◊ô ◊í◊ù ◊ß◊©◊î ◊î◊®◊ë◊î ◊ô◊ï◊™◊®. ◊û◊ë◊ê◊° ◊õ◊ô ◊î◊ß◊ï◊®◊° ◊ê◊û◊ï◊® ◊ú◊î◊¢◊ú◊ï◊™ ◊ê◊™ ◊î◊û◊û◊ï◊¶◊¢ ◊ê◊ë◊ú ◊ë◊í◊ì◊ï◊ú ◊ê◊ô◊ü ◊û◊î ◊ú◊ì◊ê◊ï◊í ◊û◊ú◊¢◊ë◊ï◊® ◊ê◊™ ◊î◊û◊ë◊ó◊ü.

◊î◊©◊ï◊®◊î ◊î◊™◊ó◊™◊ï◊†◊î: ◊ß◊ï◊®◊° ◊ó◊û◊ï◊ì ◊ú◊°◊û◊°◊ò◊® ◊®◊ê◊©◊ï◊ü ◊ë◊™◊¢◊©◊ô◊ô◊î ◊ï◊†◊ô◊î◊ï◊ú
◊õ◊ú◊ú◊ô
◊¢◊ï◊û◊°

◊§◊ï◊®◊û◊ò ◊î◊™◊©◊ï◊ë◊î ◊©◊ú◊ö:
◊¶◊ô◊ò◊ï◊ò◊ô◊ù:
"◊õ◊ú ◊©◊ë◊ï◊¢ ◊¢◊ë◊ï◊ì◊î ◊ß◊ú◊ô◊ú◊î ◊ë◊ñ◊ï◊í◊ï◊™"
"◊õ◊ú ◊î◊ó◊ï◊û◊® ◊î◊†◊ú◊û◊ì ◊ë◊™◊®◊í◊ï◊ú◊ô◊ù ◊î◊¶◊ú◊ô◊ó ◊ú◊î◊õ◊†◊° ◊ú◊û◊ë◊ó◊ü ◊©◊ú ◊©◊¢◊™◊ô◊ô◊ù ◊ê◊ñ ◊õ◊ì◊ê◊ô ◊ú◊¢◊ß◊ï◊ë ◊ê◊ó◊®◊ô ◊û◊î ◊ú◊ï◊û◊ì◊ô◊ù ◊ë◊û◊î◊ú◊ö ◊î◊°◊û◊°◊ò◊®"
"◊ê◊§◊©◊® ◊ú◊î◊ë◊ô◊ê ◊ì◊£ ◊†◊ï◊°◊ó◊ê◊ï◊™ ◊ê◊ô◊©◊ô ◊©◊û◊õ◊ô◊†◊ô◊ù ◊ú◊ë◊ì"


◊°◊ô◊õ◊ï◊ù:
◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ù ◊û◊™◊ê◊®◊ô◊ù ◊ê◊™ ◊î◊ß◊ï◊®◊° ◊õ◊î◊§◊ï◊í◊î ◊ß◊ú◊ô◊ú◊î ◊ï◊†◊ó◊û◊ì◊î ◊û◊î◊¢◊ï◊û◊° ◊î◊û◊™◊û◊ò◊ô ◊©◊ú ◊°◊û◊°◊ò◊® ◊ê', ◊ê◊ù ◊õ◊ô ◊®◊ë◊ô◊ù ◊¶◊ô◊ô◊†◊ï ◊©◊î◊û◊®◊¶◊î ◊†◊ï◊ò◊î ◊ú◊î◊ê◊ì◊ô◊® ◊ê◊™ ◊¢◊¶◊û◊ï ◊ï◊î◊™◊®◊í◊ï◊ú◊ô◊ù ◊ú◊®◊ï◊ë ◊û◊®◊í◊ô◊©◊ô◊ù ◊û◊ô◊ï◊™◊®◊ô◊ù. ◊û◊ë◊ó◊ô◊†◊™ ◊¢◊ï◊û◊°, ◊ô◊©◊†◊ü ◊û◊ò◊ú◊ï◊™ ◊©◊ë◊ï◊¢◊ô◊ï◊™ (◊ú◊®◊ï◊ë ◊ò◊õ◊†◊ô◊ï◊™/◊û◊û◊ï◊ó◊ñ◊®◊ï◊™), ◊ê◊ö ◊©◊ô◊û◊ï ◊ú◊ë ◊©◊î◊û◊ë◊ó◊ü ◊î◊ê◊ó◊®◊ï◊ü ◊™◊ï◊ê◊® ◊õ◊¢◊û◊ï◊° ◊ï◊ê◊®◊ï◊ö ◊û◊ê◊ï◊ì ◊ë◊ô◊ó◊° ◊ú◊ñ◊û◊ü ◊î◊û◊ï◊ß◊¶◊ë, ◊ë◊†◊ô◊í◊ï◊ì ◊ú◊©◊†◊ô◊ù ◊¢◊ë◊®◊ï.
"""

TOPIC_EXTRACTION_PROMPT = """You are an expert in analyzing academic curricula.
First, translate the hebrew text to english and then:
Your Task: Extract the main topics taught in the course from the official description provided below.

Course Description:
{description}

Return a list of topics in JSON format only, with no additional text or markdown formatting:
{{"topics": ["Topic 1", "Topic 2", "Topic 3"]}}

Requirements:
1. Use English names for the topics.
2. Return between 2 to 6 topics.
"""


# ============================================================================
# MODEL LOADING
# ============================================================================

def load_model():
    """Load the LLM model with optimal settings for Colab GPU"""
    print("Loading model...")

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        token=HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != "YOUR_HF_TOKEN_HERE" else None
    )

    # Set pad token if not set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load model based on quantization availability
    if USE_QUANTIZATION and QUANTIZATION_AVAILABLE:
        print("Loading with 4-bit quantization...")
        from transformers import BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            token=HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != "YOUR_HF_TOKEN_HERE" else None,
            trust_remote_code=True
        )
    else:
        print("Loading model in standard mode...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            token=HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != "YOUR_HF_TOKEN_HERE" else None,
            trust_remote_code=True
        )

    print(f"‚úì Model loaded successfully on {model.device}")
    print(f"  Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters")

    return model, tokenizer


# ============================================================================
# LLM INFERENCE FUNCTIONS
# ============================================================================

def generate_response(model, tokenizer, prompt: str, max_tokens: int = 512) -> str:
    """Generate response from the model"""

    messages = [
        {"role": "system", "content": "You are a native Hebrew and English speaker."},
        {"role": "user", "content": prompt}
    ]

    # Format prompt for the model
    try:
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except:
        # Fallback if chat template not available
        formatted_prompt = f"<|system|>◊ê◊™◊î ◊¢◊ï◊ñ◊® ◊û◊ï◊¢◊ô◊ú ◊ï◊û◊ì◊ï◊ô◊ß.</s>\n<|user|>{prompt}</s>\n<|assistant|>"

    # Tokenize
    inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.3,  # Lower temperature for more consistent outputs
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # Decode only the new tokens
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()


# ============================================================================
# PROCESSING FUNCTIONS
# ============================================================================

def extract_review_summary(model, tokenizer, reviews: str) -> str:
    """Extract quotes and summary from student reviews"""

    if pd.isna(reviews) or not reviews.strip():
        return ""

    # Truncate reviews if too long (keep first 2000 chars for 1B model)
    reviews_text = str(reviews)[:2000]

    prompt = REVIEW_SUMMARY_PROMPT.format(reviews=reviews_text)

    print("\n" + "=" * 80)
    print("PROCESSING REVIEW SUMMARY")
    print("=" * 80)

    try:
        response = generate_response(model, tokenizer, prompt, max_tokens=400)

        print(f"LLM Output:\n{response}")
        print("=" * 80)

        # Parse the response
        summary = parse_review_response(response)
        return summary
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return ""


def parse_review_response(response: str) -> str:
    """Parse the LLM response to extract formatted summary"""

    # Try to extract quotes and summary sections
    quotes = []
    summary = ""

    # Extract quotes - look for text in quotation marks
    quote_pattern = r'"([^"]+)"'
    found_quotes = re.findall(quote_pattern, response)
    quotes = found_quotes[:4]  # Maximum 4 quotes

    # Extract summary
    if "SUMMARY:" in response:
        summary_part = response.split("SUMMARY:")[-1].strip()
        # Take first paragraph after SUMMARY:
        summary = summary_part.split("\n\n")[0].strip()
        # Remove any remaining quotes section
        summary = re.sub(r'^QUOTES:.*?\n\n', '', summary, flags=re.DOTALL)
    elif "◊°◊ô◊õ◊ï◊ù:" in response:
        summary_part = response.split("◊°◊ô◊õ◊ï◊ù:")[-1].strip()
        summary = summary_part.split("\n\n")[0].strip()
    else:
        # Fallback: take the last substantial paragraph
        paragraphs = [p.strip() for p in response.split("\n\n") if p.strip() and '"' not in p]
        if paragraphs:
            summary = paragraphs[-1]

    # Format final output
    result_parts = []
    for quote in quotes:
        # Clean quote
        quote = quote.strip()
        if len(quote) > 10:  # Only include substantial quotes
            result_parts.append(f'"{quote}"')

    if summary and len(summary) > 20:
        result_parts.append(f"\n{summary}")

    return "\n".join(result_parts) if result_parts else response[:300]


def extract_topics(model, tokenizer, description: str) -> List[str]:
    """Extract course topics from description"""

    if pd.isna(description) or not description.strip():
        return []

    prompt = TOPIC_EXTRACTION_PROMPT.format(description=description[:1000])

    print("\n" + "=" * 80)
    print("PROCESSING TOPIC EXTRACTION")
    print("=" * 80)

    try:
        response = generate_response(model, tokenizer, prompt, max_tokens=200)

        print(f"LLM Output:\n{response}")
        print("=" * 80)

        # Parse JSON response
        topics = parse_topics_response(response)
        return topics
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return []


def parse_topics_response(response: str) -> List[str]:
    """Parse the LLM response to extract topics list"""

    try:
        # Try to find JSON in the response
        json_match = re.search(r'\{[^}]+\}', response)
        if json_match:
            data = json.loads(json_match.group())
            if "topics" in data:
                topics = data["topics"]
                # Clean and validate
                topics = [t.strip() for t in topics if isinstance(t, str) and len(t.strip()) > 2]
                return topics[:6]

        # Fallback 1: Look for array format ["item", "item"]
        array_match = re.search(r'\[(.*?)\]', response)
        if array_match:
            items_str = array_match.group(1)
            items = re.findall(r'"([^"]+)"', items_str)
            if items:
                return [t.strip() for t in items if len(t.strip()) > 2][:6]

        # Fallback 2: Extract comma-separated items
        topics = []
        for line in response.split("\n"):
            if ":" in line:
                line = line.split(":", 1)[1]

            # Remove brackets and quotes
            line = re.sub(r'[\[\]{}"]', '', line)

            # Split by commas
            items = [item.strip() for item in line.split(",") if item.strip()]
            topics.extend(items)

        # Filter and deduplicate
        topics = [t for t in topics if 2 < len(t) < 50]
        seen = set()
        unique_topics = []
        for t in topics:
            t_lower = t.lower()
            if t_lower not in seen:
                seen.add(t_lower)
                unique_topics.append(t)

        return unique_topics[:6]

    except Exception as e:
        print(f"‚ö† Parsing error: {e}")
        return []


# ============================================================================
# MAIN PROCESSING PIPELINE
# ============================================================================

def process_course_data(df: pd.DataFrame, model, tokenizer, max_courses: int = None) -> pd.DataFrame:
    """Process all courses in the dataframe"""

    # Limit number of courses if specified
    if max_courses:
        df = df.head(max_courses)

    print(f"\nProcessing {len(df)} courses...")

    # Initialize new columns
    df['Review_summary'] = ""
    df['Course_Topic'] = None

    for idx, row in df.iterrows():
        print(f"\n{'=' * 80}")
        print(f"Processing Course {idx + 1}/{len(df)}")
        print(f"Title: {row.get('title', 'Unknown')}")
        print(f"Course ID: {row.get('course_id', 'N/A')}")
        print(f"{'=' * 80}")

        # Extract review summary
        if 'all_reviews' in df.columns and pd.notna(row['all_reviews']) and str(row['all_reviews']).strip():
            try:
                summary = extract_review_summary(model, tokenizer, row['all_reviews'])
                df.at[idx, 'Review_summary'] = summary
                print(f"‚úì Review summary extracted ({len(summary)} chars)")
            except Exception as e:
                print(f"‚ùå Error processing reviews: {e}")
                df.at[idx, 'Review_summary'] = ""
        else:
            print("‚äò No reviews available")
            df.at[idx, 'Review_summary'] = ""

        # Extract topics
        if 'description' in df.columns and pd.notna(row['description']) and str(row['description']).strip():
            try:
                topics = extract_topics(model, tokenizer, row['description'])
                df.at[idx, 'Course_Topic'] = topics
                print(f"‚úì Topics extracted: {topics}")
            except Exception as e:
                print(f"‚ùå Error extracting topics: {e}")
                df.at[idx, 'Course_Topic'] = []
        else:
            print("‚äò No description available")
            df.at[idx, 'Course_Topic'] = []

        # Save progress periodically
        if (idx + 1) % 5 == 0:
            df.to_csv('courses_data_progress.csv', index=False, encoding='utf-8')
            print(f"\nüíæ Progress saved after {idx + 1} courses")

    return df


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main(test_mode: bool = False):
    """Main execution function"""

    print("=" * 80)
    print("COURSE REVIEW ANALYSIS PIPELINE")
    print("=" * 80)

    # Load data
    print("\nLoading CSV file...")
    df = pd.read_csv('courses_data_before_llm.csv', encoding='utf-8')
    print(f"‚úì Loaded {len(df)} courses")
    print(f"  Columns: {list(df.columns)}")

    # Validate required columns
    required_cols = ['course_id', 'title']
    optional_cols = ['description', 'all_reviews']

    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        print(f"‚ùå Missing required columns: {missing}")
        return

    for col in optional_cols:
        if col in df.columns:
            non_empty = df[col].notna().sum()
            print(f"  {col}: {non_empty}/{len(df)} courses have data")

    # Load model
    model, tokenizer = load_model()

    # Process data (test mode: only 3 courses)
    max_courses = 3 if test_mode else None
    if test_mode:
        print(f"\n‚ö† TEST MODE: Processing only {max_courses} courses")

    df_processed = process_course_data(df, model, tokenizer, max_courses)

    # Save results
    output_file = 'courses_data_processed.csv'
    df_processed.to_csv(output_file, index=False, encoding='utf-8')
    print(f"\n{'=' * 80}")
    print(f"‚úì Processing complete! Results saved to: {output_file}")
    print(f"{'=' * 80}")

    # Display sample results
    print("\nüìä Sample Results:")
    print("-" * 80)
    for idx in range(min(3, len(df_processed))):
        row = df_processed.iloc[idx]
        print(f"\n{idx + 1}. {row.get('title', 'Unknown')}")
        print(f"   Topics: {row['Course_Topic']}")
        summary = row['Review_summary']
        if summary:
            print(f"   Summary preview: {summary[:150]}...")
        else:
            print(f"   Summary: (empty)")

    return df_processed


# Run in test mode first (set to False for full run)
if __name__ == "__main__":
    # Set to True to test on 3 courses first
    TEST_MODE = True

    result_df = main(test_mode=TEST_MODE)

    if TEST_MODE:
        print("\n" + "=" * 80)
        print("‚úì Test completed successfully!")
        print("  To process all courses, set TEST_MODE = False")
        print("=" * 80)