import os
from dotenv import load_dotenv
from pinecone import Pinecone
from google import genai
import json
from google.genai import types
from src.knowledgebase import embed_query
# Initialize Google GenAI client




def get_pinecone():
    """Initialize Pinecone client"""
    load_dotenv()
    api_key = os.getenv("PINECONE_API_KEY")

    if not api_key:
        raise ValueError("No API key found. Please check your .env file.")

    pc = Pinecone(api_key=api_key)
    return pc


def get_index_by_semester(semester_name):
    """Get Pinecone index for specific semester"""
    pc = get_pinecone()
    kb_name = os.getenv(semester_name)

    if not kb_name:
        raise ValueError(f"No index found for semester: {semester_name}")

    index = pc.Index(host=kb_name)
    return index





def search_reviews(query, semester_name="WINTER_2025_2026_RAG", top_k=15):
    """
    Search for relevant course reviews based on user query

    Args:
        query: User's question
        semester_name: Semester identifier for the index
        top_k: Number of results to return

    Returns:
        List of relevant course reviews with metadata
    """
    try:
        print(f"\n{'=' * 80}")
        print(f"ğŸ” SEARCHING REVIEWS")
        print(f"{'=' * 80}")
        print(f"Query: {query}")
        print(f"Semester: {semester_name}")
        print(f"Top K: {top_k}")

        # Get index
        index = get_index_by_semester(semester_name)
        print(f"âœ… Connected to index: {semester_name}")

        # Generate query embedding
        query_embedding = embed_query(query)

        if query_embedding is None:
            print("âŒ Failed to generate embedding")
            return []

        print(f"âœ… Generated embedding (dim: {len(query_embedding)})")

        # Search in Pinecone
        results = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True
        )

        print(f"\nğŸ“Š SEARCH RESULTS: Found {len(results.matches)} matches")
        print(f"{'-' * 80}")

        # Extract relevant information
        context_chunks = []
        for i, match in enumerate(results.matches, 1):
            course_id = match.metadata.get('course_id', 'N/A')
            course_title = match.metadata.get('title', 'N/A')
            review_text = match.metadata.get('chunk_text', '')
            score = match.score

            print(f"\n[{i}] Score: {score:.4f}")
            print(f"    Course: {course_title} ({course_id})")
            print(f"    Review preview: {review_text[:150]}...")

            context_chunks.append({
                'course_id': course_id,
                'course_title': course_title,
                'review_text': review_text,
                'score': score
            })

        print(f"\n{'=' * 80}\n")
        return context_chunks

    except Exception as e:
        print(f"âŒ Error searching reviews: {e}")
        import traceback
        traceback.print_exc()
        return []


def build_context(search_results):
    """Build context string from search results for the LLM"""
    if not search_results:
        print("âš ï¸ No search results to build context from")
        return "×œ× × ××¦××• ×‘×™×§×•×¨×•×ª ×¨×œ×•×•× ×˜×™×•×ª."

    context_parts = []

    for i, result in enumerate(search_results, 1):
        course_title = result['course_title']
        course_id = result['course_id']
        review = result['review_text']



        context_parts.append(f"\n--- ×‘×™×§×•×¨×ª {i} | {course_title} ({course_id}) ---\n{review}")

    full_context = "\n".join(context_parts)

    print(f"\n{'=' * 80}")
    print(f"ğŸ“ BUILT CONTEXT FOR LLM")
    print(f"{'=' * 80}")
    print(f"Total context length: {len(full_context)} characters")
    print(f"Number of reviews: {len(search_results)}")
    print(f"\nContext preview (first 500 chars):")
    print(full_context[:500])
    print(f"{'=' * 80}\n")

    return full_context


def chat_with_assistant(user_message, semester_name="WINTER_2025_2026_RAG", conversation_history=None):
    """
    Main chat function - handles user queries using RAG

    Args:
        user_message: User's question
        semester_name: Semester identifier
        conversation_history: Previous conversation (optional)

    Returns:
        dict with 'response' and 'sources'
    """
    load_dotenv()

    # 2. Get the key securely
    CHAT_MODEL = os.getenv("CHAT_MODEL")
    GOOGLE_API_KEY = os.getenv("GOOLGE_API_KEY")
    genai_client = genai.Client(api_key=GOOGLE_API_KEY)
    try:
        print(f"\n{'#' * 80}")
        print(f"ğŸ’¬ NEW CHAT REQUEST")
        print(f"{'#' * 80}")
        print(f"User message: {user_message}")
        print(f"Semester: {semester_name}")
        print(f"Conversation history length: {len(conversation_history) if conversation_history else 0}")

        # Search for relevant reviews
        search_results = search_reviews(user_message, semester_name, top_k=15)

        # Build context from search results
        context = build_context(search_results)

        # Build conversation history
        if conversation_history is None:
            conversation_history = []

        # System prompt focused on answering questions
        system_prompt = """××ª×” ×¢×•×–×¨ ×•×™×¨×˜×•××œ×™ ×©×œ CheeseSpoon - ××¢×¨×›×ª ×”××œ×¦×•×ª ×§×•×¨×¡×™× ×©×œ ×”×˜×›× ×™×•×Ÿ.
×”×ª×¤×§×™×“ ×©×œ×š ×”×•× ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª ×¡×¤×¦×™×¤×™×•×ª ×©×œ ×¡×˜×•×“× ×˜×™× ×¢×œ ×§×•×¨×¡×™×, ×‘×”×ª×‘×¡×¡ ×¢×œ ×‘×™×§×•×¨×•×ª ×©×œ ×¡×˜×•×“× ×˜×™× ×©×œ××“×• ××ª ×”×§×•×¨×¡×™×.

×”× ×—×™×•×ª ×—×©×•×‘×•×ª:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×™×©×™×¨×” ×•××“×•×™×§×ª ×¢×œ ×”×©××œ×” ×©× ×©××œ×”
2. ×”×ª×‘×¡×¡ ××š ×•×¨×§ ×¢×œ ×”××™×“×¢ ×©× ×™×ª×Ÿ ×œ×š ××”×‘×™×§×•×¨×•×ª - ××œ ×ª××¦×™× ××™×“×¢
3. ×× ×”×©××œ×” ×”×™× ×¢×œ × ×•×©× ×¡×¤×¦×™×¤×™ (×œ××©×œ: "×”×× ×™×© ×©×™×¢×•×¨×™ ×‘×™×ª?", "××” ××•××¨×™× ×¢×œ ×”××¨×¦×”?", "×›××” ×–××Ÿ ×œ×•×§×— ×œ×”×›×™×Ÿ ×œ××‘×—×Ÿ?") - ×—×¤×© ××ª ×”××™×“×¢ ×”×¨×œ×•×•× ×˜×™ ×‘×‘×™×§×•×¨×•×ª ×•×¢× ×” ×‘××•×¤×Ÿ ×××•×§×“
4. ×›×©××©×•×•×™× ×‘×™×Ÿ ×§×•×¨×¡×™× - ×”×¦×’ ××ª ×”×”×‘×“×œ×™× ×”×¡×¤×¦×™×¤×™×™× ×©× ×©××œ×• (×¢×•××¡, ×§×•×©×™, ××™×›×•×ª ×”×•×¨××” ×•×›×•')
5. ×× ××™×Ÿ ××™×“×¢ ×¢×œ ×”× ×•×©× ×”×¡×¤×¦×™×¤×™ ×‘×‘×™×§×•×¨×•×ª - ×××¨ ×–××ª ×‘×›× ×•×ª: "×œ× ××¦××ª×™ ××™×“×¢ ×¢×œ × ×•×©× ×–×” ×‘×‘×™×§×•×¨×•×ª"
6. ×ª×Ÿ ×ª×©×•×‘×” ×ª××¦×™×ª×™×ª ××‘×œ ××œ××” - ××œ ×ª×¡×›× ×›×œ×œ×™ ××œ× ×¢× ×” ×¢×œ ×”×©××œ×” ×”×§×•× ×§×¨×˜×™×ª
7. ×× ×™×© ×“×¢×•×ª ×©×•× ×•×ª ×‘×‘×™×§×•×¨×•×ª - ×”×¦×’ ××ª ××’×•×•×Ÿ ×”×“×¢×•×ª

×“×•×’×××•×ª ×œ×¡×•×’ ×”×©××œ×•×ª ×©××ª×” ×¦×¨×™×š ×œ×¢× ×•×ª ×¢×œ×™×”×Ÿ:
- "××” ××•××¨×™× ×¢×œ ×¢×•××¡ ×”×¢×‘×•×“×” ×‘×§×•×¨×¡ X?"
- "×”×× ×™×© ××‘×—×Ÿ ××• ×©×–×” ×¤×¨×•×™×§×˜?"
- "××” ×”×¡×˜×•×“× ×˜×™× ××•××¨×™× ×¢×œ ×”××¨×¦×” Y?"
- "×›××” ×§×©×” ×”×§×•×¨×¡ ×”×–×”?"
- "×”×× ×›×“××™ ×œ×§×—×ª ××ª ×”×§×•×¨×¡ X ××• Y?"
- "××” ×¦×¨×™×š ×œ×“×¢×ª ××¨××© ×‘×©×‘×™×œ ×”×§×•×¨×¡?"
- "××™×š × ×¨××” ×”××‘×—×Ÿ?"

×–×›×•×¨: ××ª×” ×œ× ××¡×›× ××ª ×”×§×•×¨×¡ - ××ª×” ×¢×•× ×” ×¢×œ ×©××œ×•×ª ×¡×¤×¦×™×¤×™×•×ª!"""

        # Build the prompt
        user_prompt = f"""×©××œ×ª ×”×¡×˜×•×“× ×˜: {user_message}

×‘×™×§×•×¨×•×ª ×¨×œ×•×•× ×˜×™×•×ª ××”×××’×¨:
{context}

×‘×‘×§×©×” ×¢× ×” ×¢×œ ×”×©××œ×” ×‘×”×ª×‘×¡×¡ ×¢×œ ×”×‘×™×§×•×¨×•×ª. 
×× ×”×©××œ×” ××©×•×•×” ×‘×™×Ÿ ×§×•×¨×¡×™× - ×”×¦×’ ××ª ×”×”×‘×“×œ×™× ×”×¡×¤×¦×™×¤×™×™×.
×× ××™×Ÿ ××™×“×¢ ×¨×œ×•×•× ×˜×™ - ×××¨ ×–××ª."""

        print(f"\n{'=' * 80}")
        print(f"ğŸ¤– CALLING LLM")
        print(f"{'=' * 80}")
        print(f"Model: {CHAT_MODEL}")
        print(f"Temperature: 0.4")
        print(f"Max tokens: 800")
        print(f"\nFull prompt (first 800 chars):")
        print(user_prompt[:800])
        print(f"{'=' * 80}\n")

        # Call Google GenAI API
        response = genai_client.models.generate_content(
            model=CHAT_MODEL,
            contents=user_prompt,
            config={
                "system_instruction": system_prompt,
                "temperature": 0.4,  # Lower temperature for more focused answers
                "max_output_tokens": 5000,
            }
        )

        assistant_response = response.text

        print(f"\n{'=' * 80}")
        print(f"âœ… LLM RESPONSE RECEIVED")
        print(f"{'=' * 80}")
        print(f"Response length: {len(assistant_response)} characters")
        print(f"Response preview (first 300 chars):")
        print(assistant_response[:300])
        print(f"{'=' * 80}\n")

        # Prepare sources for citation
        sources = []
        seen_courses = {}

        for result in search_results:
            course_key = f"{result['course_id']}_{result['course_title']}"

            # Only show each course once, with highest relevance score
            if course_key not in seen_courses:
                seen_courses[course_key] = {
                    'course_id': result['course_id'],
                    'course_title': result['course_title'],

                    'relevance_score': round(result['score'] * 100, 1)
                }

        # Take top 4 unique courses
        sources = list(seen_courses.values())[:4]

        print(f"ğŸ“š SOURCES PREPARED: {len(sources)} unique courses")
        for i, source in enumerate(sources, 1):
            print(f"  [{i}] {source['course_title']} ({source['course_id']}) - {source['relevance_score']}%")

        print(f"\n{'#' * 80}\n")

        return {
            'response': assistant_response,
            'sources': sources,
            'success': True
        }

    except Exception as e:
        print(f"\nâŒ ERROR IN CHAT_WITH_ASSISTANT")
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        print(f"{'#' * 80}\n")

        return {
            'response': f"××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××”: {str(e)}",
            'sources': [],
            'success': False
        }


def answer_course_question(course_id, question, semester_name="WINTER_2025_2026_RAG"):
    """
    Answer a specific question about a particular course

    Args:
        course_id: Course ID
        question: Specific question about the course
        semester_name: Semester identifier

    Returns:
        Answer text
    """
    try:
        print(f"\n{'=' * 80}")
        print(f"â“ COURSE-SPECIFIC QUESTION")
        print(f"{'=' * 80}")
        print(f"Course ID: {course_id}")
        print(f"Question: {question}")

        # Create targeted query
        query = f"×§×•×¨×¡ {course_id} {question}"
        print(f"Formatted query: {query}")

        # Search for reviews of this specific course
        search_results = search_reviews(query, semester_name, top_k=15)

        # Filter to only this course and high relevance
        course_reviews = [
            r for r in search_results
            if r['course_id'] == str(course_id) and r['score'] > 0.3
        ]

        print(f"Filtered to {len(course_reviews)} reviews for course {course_id}")

        if not course_reviews:
            print("âš ï¸ No relevant reviews found")
            return "×œ× × ××¦× ××™×“×¢ ×¡×¤×¦×™×¤×™ ×¢×œ × ×•×©× ×–×” ×‘×‘×™×§×•×¨×•×ª."

        # Build context
        context = build_context(course_reviews)

        # Create focused prompt
        prompt = f"""×©××œ×” ×¢×œ ×§×•×¨×¡ {course_id}: {question}

×‘×™×§×•×¨×•×ª ×¨×œ×•×•× ×˜×™×•×ª:
{context}

×¢× ×” ×¢×œ ×”×©××œ×” ×‘×¦×•×¨×” ×™×©×™×¨×” ×•×¡×¤×¦×™×¤×™×ª ×‘×”×ª×‘×¡×¡ ×¢×œ ×”×‘×™×§×•×¨×•×ª.
××œ ×ª×¡×›× ××ª ×›×œ ×”×§×•×¨×¡ - ×¨×§ ×¢× ×” ×¢×œ ×”×©××œ×” ×”×¡×¤×¦×™×¤×™×ª ×©× ×©××œ×”.
×× ××™×Ÿ ××™×“×¢ - ×××¨ "×œ× ××¦××ª×™ ××™×“×¢ ×¢×œ ×›×š ×‘×‘×™×§×•×¨×•×ª"."""

        print(f"\nğŸ¤– Calling LLM for course-specific answer...")

        response = genai_client.models.generate_content(
            model=CHAT_MODEL,
            contents=prompt,
            config={
                "temperature": 0.3,
                "max_output_tokens": 400,
            }
        )

        answer = response.text
        print(f"âœ… Answer received: {answer[:200]}...")
        print(f"{'=' * 80}\n")

        return answer

    except Exception as e:
        print(f"âŒ Error answering course question: {e}")
        import traceback
        traceback.print_exc()
        return "×œ× × ×™×ª×Ÿ ×œ×¢× ×•×ª ×¢×œ ×”×©××œ×” ×›×¨×’×¢."


# For testing
if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("ğŸ§ª TESTING RAG Q&A SYSTEM")
    print("=" * 80 + "\n")

    # Test 1: Specific question about workload
    print("\n" + "ğŸ”¬ TEST 1: Question about workload")
    result1 = chat_with_assistant("××” ××•××¨×™× ×¢×œ ×¢×•××¡ ×”×¢×‘×•×“×” ×‘×§×•×¨×¡×™ ×›×œ×›×œ×”?")
    print("\nğŸ“‹ FINAL RESULT:")
    print("Response:", result1['response'])
    print("Sources:", result1['sources'])
    print("\n" + "=" * 80 + "\n")

    # Test 2: Question about exams
    print("\n" + "ğŸ”¬ TEST 2: Question about exams")
    result2 = chat_with_assistant("××™×š × ×¨××™× ×”××‘×—× ×™× ×‘×§×•×¨×¡×™× ×¢× ×¤×¨×¥ ×—×•×‘×‘?")
    print("\nğŸ“‹ FINAL RESULT:")
    print("Response:", result2['response'])
    print("Sources:", result2['sources'])
    print("\n" + "=" * 80 + "\n")